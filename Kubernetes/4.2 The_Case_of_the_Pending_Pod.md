# 4.2 : Memory Limit Exceeded â†’ **OOMKilled**

https://drive.google.com/file/d/1l0bQJNRRjrxzuIn0dwAb4AMbM67MIgrp/view?usp=sharing

### ğŸ” YAML Breakdown

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nnappone
  namespace: learning
  labels:
    app: nnappone
spec:
  containers:
    - name: crackone-app
      image: lovelearnlinux/stress:latest     # â† Special image with `stress` tool
      resources:
        requests:
          memory: "150Mi"   # Scheduler reserves 150Mi
        limits:
          memory: "250Mi"   # Hard ceiling = 250Mi
      command: ["stress"]
      args: ["--vm", "1", "--vm-bytes", "275M", "--vm-hang", "1"]

```

> ğŸ’¥ What the stress command does:
> 
> - `-vm 1` â†’ Start 1 worker
> - `-vm-bytes 275M` â†’ **Allocate 275 MB of memory**
> - `-vm-hang 1` â†’ Lock the memory (donâ€™t release it)

> âš ï¸ Conflict:
> 
> 
> Pod tries to use **275 MB** > **limit = 250 MiB** â†’ **Killed by OOM Killer!**
> 

> ğŸ” Note: 275M (Megabytes = 1000Â²) â‰ˆ 262 MiB (Mebibytes = 1024Â²) â†’ still > 250 MiB
> 

---

### ğŸ“Œ What Happens at Runtime?

| Phase | What Occurs |
| --- | --- |
| **1. Scheduling** | âœ… Success! Node has â‰¥150Mi free â†’ Pod starts |
| **2. Container Runs** | `stress` allocates memory â†’ reaches ~262 MiB |
| **3. Memory > 250MiB** | **Container Runtime (containerd/docker) triggers OOM Killer** |
| **4. Pod Status** | Changes to **`OOMKilled`** â†’ restarts (if `restartPolicy: Always`) |
| **5. Event Logged** | `Exit Code 137` = **128 + 9** â†’ signal **SIGKILL** |

> ğŸ’¡ Key Difference from CPU:
> 
> - **CPU overuse** â†’ **throttled** (slowed, but survives)
> - **Memory overuse** â†’ **killed immediately** (no second chances!)

---

### ğŸ§ª Lab: Trigger & Diagnose OOMKilled

### ğŸ”§ Steps

```bash
# 1. Create namespace
kubectl create namespace learning

# 2. Apply the Pod
kubectl apply -f pod-with-memory-exceed.yml

# 3. Watch it crash
kubectl get pods -n learning -w

# âœ… Youâ€™ll see:
# nnappone   0/1   OOMKilled   1 (or more)   10s

# 4. Describe Pod â†’ look for OOM event
kubectl describe pod nnappone -n learning

# ğŸ” Critical lines in "Events":
#   Last State:     Terminated
#     Reason:       OOMKilled
#     Exit Code:    137
#     ...
#   Restart Count:  2+

# 5. Check logs (may be empty if killed instantly)
kubectl logs nnappone -n learning --previous

# 6. Clean up
kubectl delete pod nnappone -n learning
kubectl delete namespace learning

```

---

### ğŸ” Understanding Exit Code 137

| Signal | Meaning |
| --- | --- |
| **SIGKILL (9)** | Force-kill process (canâ€™t be caught/ignored) |
| **Exit Code = 128 + 9 = 137** | Standard way to indicate **killed by SIGKILL** |

> âœ… In Kubernetes:
> 
> 
> **Exit Code 137 + Reason: OOMKilled** = **memory limit exceeded**
> 

---

### ğŸ› ï¸ How to Fix It?

### âœ… Option 1: Increase Memory Limit

```yaml
resources:
  limits:
    memory: "300Mi"   # > 275M (~262Mi)

```

### âœ… Option 2: Reduce Memory Usage

```yaml
args: ["--vm-bytes", "200M"]  # Stay under 250Mi

```

### âœ… Option 3: Add Memory Monitoring

Use **Prometheus + Grafana** to track:

- `container_memory_usage_bytes`
- `container_memory_working_set_bytes`

> ğŸ“Š Rule of Thumb:
> 
> 
> Set `limits` â‰ˆ **peak usage + 20% buffer**
> 
> Set `requests` â‰ˆ **average usage**
> 

---

### â“ Common Questions

**Q: Why not just set huge memory limits?**

A: Wastes cluster capacity â†’ fewer Pods per node â†’ higher costs.

**Q: Can I catch OOM in app logs?**

A: âŒ No â€” the process is **force-killed** by the kernel. Use **metrics** instead.

**Q: Does `restartPolicy: Never` prevent restarts?**

A: Yes â€” but the Pod stays in **`CrashLoopBackOff`** if it fails on first run.

**Q: Whatâ€™s the difference between `Mi` and `M`?**

A:

- `250Mi` = 250 Ã— 1024Â² = **262,144,000 bytes**
- `275M` = 275 Ã— 1000Â² = **275,000,000 bytes** â†’ **still exceeds 250Mi!**

---

### ğŸ“Š Best Practices for Memory Management

| Practice | Why |
| --- | --- |
| **Always set `requests` and `limits`** | Avoid BestEffort, control scheduling |
| **Monitor real usage** | Donâ€™t guess â€” use `kubectl top pods` |
| **Test memory limits** | Use `stress` image in staging |
| **Use liveness/readiness probes** | Detect memory leaks early |

```bash
# See real memory usage
kubectl top pods -n learning

```

---

### â¡ï¸ Summary

âœ… **Memory overuse â†’ OOMKilled** (immediate kill, exit code 137).

ğŸ” Diagnose with: `kubectl describe pod` â†’ **"Reason: OOMKilled"**.

ğŸ“Š Memory enforcement is **hard limit** (unlike CPU throttling).

ğŸ› ï¸ Fix by: **increasing limit** or **reducing usage**.

ğŸ§ª Use `stress` image to **test limits safely**.