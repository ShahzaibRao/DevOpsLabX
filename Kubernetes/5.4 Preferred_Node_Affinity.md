# 5.4 : Preferred Node Affinity (Soft Constraints)

https://drive.google.com/file/d/1kvdiur755gq4MtlglNvrHQ6krRCB2K7l/view?usp=sharing

### ğŸ” YAML Breakdown

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nnappone
  namespace: learning
  labels:
    app: nnappone
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1                          # â† Importance (1-100)
        preference:
          matchExpressions:
          - key: size
            operator: In
            values:
            - small
  containers:
    - name: cracokone-app
      image: nginx
      resources:
        requests:
          memory: "300Mi"
        limits:
          memory: "500Mi"

```

> ğŸ”‘ Key Concepts:
> 
> - **`preferredDuringScheduling...`** â†’ **soft rule** (scheduler tries, but wonâ€™t block)
> - **`weight: 1`** â†’ how much to **favor** this rule (1 = low, 100 = high)
> - **`preference`** â†’ what node attributes are preferred

> ğŸ¯ Behavior:
> 
> - If **any `size=small` node is available** â†’ Pod **likely** runs there
> - If **no `size=small` node** â†’ Pod **still runs** on another node
> - Multiple preferences? Scheduler **scores** nodes and picks the best

---

### ğŸ“Œ How Preferred Affinity Works

| Scenario | Outcome |
| --- | --- |
| âœ… **`size=small` node exists + has resources** | Pod **scheduled there** (preferred) |
| âŒ **No `size=small` node** | Pod **scheduled elsewhere** (no failure!) |
| ğŸŸ¡ **Multiple preferred rules** | Scheduler **scores** nodes â†’ picks highest total weight |

> ğŸ’¡ Weight Range: 1 to 100
> 
> - Use higher weights for **stronger preferences** (e.g., `weight: 80` for SSD nodes)

---

### ğŸ§ª k3s Lab: Preferred Affinity in Action

> âœ… Assumption: You have at least 2 worker nodes in your k3s cluster.
> 

### ğŸ”§ Step 1: Label One Node as `small`, Leave Others Unlabeled

```bash
# 1. List your k3s nodes
kubectl get nodes

# Example:
# NAME         STATUS   ROLES    AGE   VERSION
# k3s-master   Ready    master   2d    v1.28.5+k3s1
# k3s-node1    Ready    <none>   2d    v1.28.5+k3s1
# k3s-node2    Ready    <none>   2d    v1.28.5+k3s1

# 2. Label ONLY ONE node as "small"
kubectl label node k3s-node1 size=small

# 3. Verify
kubectl get nodes --show-labels | grep -E "k3s-node1|k3s-node2"
# k3s-node1 ... size=small
# k3s-node2 ... (no size label)

```

### ğŸ”§ Step 2: Deploy the Pod

```bash
# 1. Create namespace
kubectl create namespace learning

# 2. Apply Pod
kubectl apply -f pod-with-preferred-node-affinity.yml

# 3. Check where it runs
kubectl get pods -n learning -o wide

# âœ… Expected: Runs on k3s-node1 (the "small" node)
# But if k3s-node1 is full, it might run on k3s-node2 â€” that's OK!

```

### ğŸ”§ Step 3: Test Fallback Behavior

> ğŸ’¡ Force fallback: Temporarily taint the small node so itâ€™s unschedulable.
> 

```bash
# 1. Taint the "small" node
kubectl taint node k3s-node1 test=unschedulable:NoSchedule

# 2. Deploy a SECOND Pod (same spec)
kubectl run nnappone2 -n learning --image=lovelearnlinux/webserver:v1 --restart=Never \\
  --overrides='
{
  "spec": {
    "affinity": {
      "nodeAffinity": {
        "preferredDuringSchedulingIgnoredDuringExecution": [
          {
            "weight": 1,
            "preference": {
              "matchExpressions": [
                { "key": "size", "operator": "In", "values": ["small"] }
              ]
            }
          }
        ]
      }
    }
  }
}'

# 3. Check placement
kubectl get pods -n learning -o wide

# âœ… Expected: nnappone2 runs on k3s-node2 (fallback!)

```

### ğŸ”§ Step 4: Clean Up

```bash
kubectl delete pod nnappone nnappone2 -n learning
kubectl taint node k3s-node1 test:NoSchedule-  # remove taint
kubectl label node k3s-node1 size-             # remove label
kubectl delete namespace learning

```

---

### ğŸ†š Required vs Preferred Affinity

| Feature | `required...` | `preferred...` |
| --- | --- | --- |
| **Constraint Type** | Hard (must match) | Soft (try to match) |
| **Scheduling Failure?** | âœ… Yes (if no match) | âŒ No (always schedules) |
| **Use Case** | "Must run on GPU" | "Prefer SSD, but OK on HDD" |
| **Weight** | N/A | 1â€“100 (higher = stronger preference) |

> ğŸ¯ Best Practice:
> 
> 
> Use **`preferred`** for **optimization**, **`required`** for **hard requirements**.
> 

---

### ğŸ’¡ Real-World Example: Cost Optimization

```yaml
affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      preference:
        matchExpressions:
        - key: node.kubernetes.io/instance-type
          operator: In
          values: [t3.small]   # cheap spot instances
    - weight: 50
      preference:
        matchExpressions:
        - key: topology.kubernetes.io/zone
          operator: In
          values: [us-east-1a]

```

> âœ… Meaning:
> 
> 
> "Strongly prefer cheap instances, and somewhat prefer zone A."
> 

---

### â¡ï¸ Summary

âœ… **`preferredDuringScheduling...`** = **soft constraint** (never blocks scheduling).

âœ… Uses **`weight` (1â€“100)** to express preference strength.

âœ… **Falls back gracefully** if no preferred node is available.

ğŸ” In k3s: Works perfectly across **real multi-node clusters**.

ğŸ› ï¸ Combine with **taints** to simulate node unavailability.