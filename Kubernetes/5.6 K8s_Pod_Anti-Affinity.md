# 5.6 : Pod Anti-Affinity (Spread Pods for High Availability)

https://drive.google.com/file/d/1PkS8KYjSKCmftUz8dvNEqgh8uSNTLKaH/view?usp=sharing

---

### ‚úÖ : True Anti-Affinity

```yaml

apiVersion: v1
kind: Pod
metadata:
  name: nnappone
  namespace: learning
  labels:
    app: nnappone
spec:
  containers:
    - name: crackone-app
      image: nginx
      resources:
        requests:
          memory: "300Mi"
        limits:
          memory: "500Mi"
  affinity:
    podAntiAffinity:  # ‚Üê CHANGED FROM podAffinity!
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - nnweb
        topologyKey: "kubernetes.io/hostname"

```

> üîë Key Fix:
> 
> 
> `podAffinity` ‚Üí **`podAntiAffinity`**
> 

---

### üìå How Pod Anti-Affinity Works

| Field | Purpose |
| --- | --- |
| **`podAntiAffinity`** | "Avoid nodes that match this rule" |
| **`labelSelector`** | Select **existing Pods** to avoid |
| **`topologyKey`** | Scope of avoidance:<br>‚Ä¢ `kubernetes.io/hostname` ‚Üí **same node**<br>‚Ä¢ `topology.kubernetes.io/zone` ‚Üí **same AZ** |

> üéØ Your Rule:
> 
> 
> *"Do **NOT** schedule this Pod on any node that already runs a Pod with `app=nnweb`."*
> 

> üí° Use Case:
> 
> - Spread replicas across nodes ‚Üí **high availability**
> - Avoid noisy neighbors ‚Üí **performance isolation**

---

### üß™ k3s Lab: Enforce Pod Anti-Affinity

> ‚úÖ Assumption: You have at least 2 worker nodes in your k3s cluster.
> 

### üîß Step 1: Deploy the First Pod (`nnweb`)

> üí° You‚Äôll need pod-for-anti-affinity.yml ‚Äî let‚Äôs assume it looks like this:
> 

```yaml
# pod-for-anti-affinity.yml
apiVersion: v1
kind: Pod
meta
  name: nnweb
  namespace: learning
  labels:
    app: nnweb          # ‚Üê Critical: this label is targeted
spec:
  containers:
    - name: web
      image: nginx

```

Now deploy it:

```bash
# 1. Create namespace
kubectl create namespace learning

# 2. Deploy first Pod
kubectl apply -f pod-for-anti-affinity.yml

# 3. Check where it runs
kubectl get pods -n learning -o wide
# Example: nnweb runs on k3s-node1

```

### üîß Step 2: Deploy the Anti-Affinity Pod

```bash
# Apply the CORRECTED anti-affinity Pod
kubectl apply -f pod-with-anti-pod-affinity.yml

# Check placement
kubectl get pods -n learning -o wide

# ‚úÖ Expected:
# nnweb      ‚Üí k3s-node1
# nnappone   ‚Üí k3s-node2 (NOT on same node!)

```

### üîß Step 3: Test Failure (Only 1 Node Available)

> üí° Temporarily drain the second node to simulate single-node scenario.
> 

```bash
# 1. Cordon k3s-node2 (mark unschedulable)
kubectl cordon k3s-node2

# 2. Delete nnappone and redeploy
kubectl delete pod nnappone -n learning
kubectl apply -f pod-with-anti-pod-affinity.yml

# 3. Check status
kubectl get pods -n learning
# ‚Üí nnappone = Pending

# 4. Describe to confirm
kubectl describe pod nnappone -n learning
# Events: 0/2 nodes available: 1 node(s) had taint..., 1 node(s) didn't match pod anti-affinity rules.

```

> üîç Why?
> 
> - Only `k3s-node1` is schedulable
> - But it already runs `app=nnweb` ‚Üí **anti-affinity blocks scheduling**

### üîß Step 4: Clean Up

```bash
kubectl delete pod nnappone nnweb -n learning
kubectl uncordon k3s-node2  # if you cordoned it
kubectl delete namespace learning

```

---

### üÜö Pod Affinity vs Anti-Affinity

| Type | Use Case | Example |
| --- | --- | --- |
| **`podAffinity`** | Co-locate Pods | "Run cache next to app for low latency" |
| **`podAntiAffinity`** | Spread Pods | "Don‚Äôt run two replicas on same node" |

> ‚úÖ Anti-affinity is FAR more common in production (for HA).
> 

---

### üí° Pro Tips for k3s

1. **Always use `topologyKey: kubernetes.io/hostname`** for node-level spreading.
2. For **zone-level HA** (if you had multi-AZ k3s):
    
    ```yaml
    topologyKey: topology.kubernetes.io/zone
    
    ```
    
3. Combine with **`preferredDuringScheduling...`** for soft anti-affinity:
    
    ```yaml
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector: { matchLabels: { app: myapp } }
          topologyKey: kubernetes.io/hostname
    
    ```
    

---

### ‚ùì Common Questions

**Q: What if no Pods match the `labelSelector`?**

A: ‚úÖ Anti-affinity rule **doesn‚Äôt apply** ‚Üí Pod can run anywhere.

**Q: Can I use anti-affinity with Deployments?**

A: ‚úÖ **YES!** This is the **standard way** to spread replicas:

```yaml
apiVersion: apps/v1
kind: Deployment
spec:
  template:
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: myapp
            topologyKey: kubernetes.io/hostname

```

**Q: Does anti-affinity work across namespaces?**

A: By default, **only same-namespace Pods** are considered.

To include all namespaces, add:

```yaml
namespaces: ["*"]  # or list specific namespaces

```

**Q: What‚Äôs the performance impact?**

A: Minimal ‚Äî scheduler evaluates this at **schedule time only**.

---

### ‚û°Ô∏è Summary

‚úÖ **`podAntiAffinity`** = **avoid nodes** running matching Pods.

‚ö†Ô∏è **`podAffinity` ‚â† `podAntiAffinity`** ‚Äî double-check your YAML!

‚úÖ Use `topologyKey: kubernetes.io/hostname` to **spread across nodes**.

üîç Essential for **high availability** in k3s multi-node clusters.

üõ†Ô∏è Always test with **‚â•2 nodes** to see anti-affinity in action.