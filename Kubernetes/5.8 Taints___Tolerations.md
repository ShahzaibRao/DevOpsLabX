# 5.8 : Taints & Tolerations (Node Repellents)

https://drive.google.com/file/d/1DNe7iyLlQPbU4PW9600TDdlNdL00FLo6/view?usp=sharing

### üîç YAML Breakdown

```yaml
apiVersion: v1
kind: Pod
metadata: 
  name: nnwebserver
  namespace: learning
spec:
  containers:
    - name: nnwebserver
      image: nginx
      resources:
        requests:
          cpu: "500m"
          memory: "128Mi"
        limits:
          cpu: "1000m"
          memory: "256Mi"
      ports:
        - containerPort: 80
          name: http
          protocol: TCP
  tolerations:
  - key: "color"
    operator: "Equal"
    value: "pink"
    effect: "NoSchedule"

```

> üîë Key Concept:
> 
> - **Taints** are applied to **nodes** ‚Üí repel Pods
> - **Tolerations** are defined in **Pods** ‚Üí allow them to **tolerate** taints

> üí° Your Pod says:
> 
> 
> *"I can run on nodes tainted with `color=pink:NoSchedule`."*
> 

---

### üìå How Taints Work

| Taint Format | Meaning |
| --- | --- |
| `key=value:effect` | Standard taint |
| **Effects** |  |
| `NoSchedule` | ‚ùå New Pods **won‚Äôt be scheduled** here (unless they tolerate) |
| `PreferNoSchedule` | üü° Scheduler will **try to avoid** (soft rule) |
| `NoExecute` | ‚ö†Ô∏è **Evict existing Pods** that don‚Äôt tolerate (after grace period) |

> ‚úÖ In your case:
> 
> - Node has: `color=pink:NoSchedule`
> - Pod has toleration for `color=pink:NoSchedule`
> ‚Üí ‚úÖ Pod **can be scheduled** on that node

---

### ‚ùó Clarifying the Comment Misconception

> üìù Comment says:
> 
> 
> *"Our pod can also go to other nodes that are NEUTRAL to taint... nodeone will accept ONLY pods with pink toleration."*
> 

‚úÖ **Partially correct**, but **incomplete**:

- **Taints do NOT restrict a node to ONLY tolerant Pods**
- They **only repel non-tolerant Pods**
- **Tolerant Pods can still run on untainted nodes!**

> üéØ Correct Behavior:
> 
> - **Untainted node**: Accepts **any Pod** (tolerant or not)
> - **Tainted node**: Accepts **only Pods that tolerate the taint**

> üí° To force a Pod onto a tainted node, combine with nodeAffinity:
> 
> 
> ```yaml
> spec:
>   affinity:
>     nodeAffinity:
>       requiredDuringSchedulingIgnoredDuringExecution:
>         nodeSelectorTerms:
>         - matchExpressions:
>           - key: kubernetes.io/hostname
>             operator: In
>             values: [k3s-gpu-node]
>   tolerations:
>   - key: "type"
>     operator: "Equal"
>     value: "gpu"
>     effect: "NoSchedule"
> 
> ```
> 

---

### üß™ k3s Lab: Taints & Tolerations in Action

> ‚úÖ Assumption: You have 2+ worker nodes in your k3s cluster.
> 

### üîß Step 1: Taint a k3s Node

```bash
# 1. List nodes
kubectl get nodes

# Example:
# NAME         STATUS   ROLES    AGE   VERSION
# k3s-master   Ready    master   2d    v1.28.5+k3s1
# k3s-node1    Ready    <none>   2d    v1.28.5+k3s1
# k3s-node2    Ready    <none>   2d    v1.28.5+k3s1

# 2. Taint k3s-node1
kubectl taint node k3s-node1 color=pink:NoSchedule

# 3. Verify
kubectl describe node k3s-node1 | grep Taints
# Taints: color=pink:NoSchedule

```

### üîß Step 2: Deploy the Tolerant Pod

```bash
# 1. Create namespace
kubectl create namespace learning

# 2. Apply Pod
kubectl apply -f pod-with-taint.yml

# 3. Check placement
kubectl get pods -n learning -o wide

# üéØ Where will it run?
# - If k3s-node1 has resources ‚Üí **may run there** (tolerates taint)
# - But could also run on k3s-node2 (untainted, accepts all Pods)

```

> üîç To force it onto k3s-node1, add node affinity (see Pro Tip below).
> 

### üîß Step 3: Test Non-Tolerant Pod (Should Avoid Tainted Node)

```bash
# Deploy a Pod WITHOUT toleration
kubectl run test-pod -n learning --image=nginx --restart=Never

# Check placement
kubectl get pods -n learning -o wide
# ‚úÖ Should NOT run on k3s-node1 (tainted)
# ‚úÖ Will run on k3s-node2 (untainted)

```

### üîß Step 4: Clean Up

```bash
kubectl delete pod nnwebserver test-pod -n learning
kubectl taint node k3s-node1 color:NoSchedule-  # remove taint
kubectl delete namespace learning

```

---

### üí° Real-World Use Cases

| Scenario | Taint Key | Effect |
| --- | --- | --- |
| **Dedicated GPU nodes** | `type=gpu` | `NoSchedule` |
| **Security-isolated nodes** | `env=prod` | `NoSchedule` |
| **Spot/preemptible nodes** | `lifecycle=spot` | `NoSchedule` |
| **Master nodes** (k3s) | `node-role.kubernetes.io/master` | `NoSchedule` |

> üîí k3s Master Taint:
> 
> 
> By default, k3s master nodes have:
> 
> ```bash
> node-role.kubernetes.io/master:NoSchedule
> 
> ```
> 
> ‚Üí That‚Äôs why **Pods don‚Äôt run on master** unless they tolerate it.
> 

---

### ‚ùì Common Questions

**Q: Can a Pod have multiple tolerations?**

A: ‚úÖ Yes! Example:

```yaml
tolerations:
- key: "color"; operator: "Equal"; value: "pink"; effect: "NoSchedule"
- key: "type"; operator: "Equal"; value: "gpu"; effect: "NoSchedule"

```

**Q: What if I omit `effect` in toleration?**

A: It tolerates **any effect** (`NoSchedule`, `NoExecute`, etc.).

**Q: Does `NoExecute` evict existing Pods?**

A: ‚úÖ Yes! After `tolerationSeconds` (if specified), or immediately.

**Q: Can I taint a node to ONLY accept specific Pods?**

A: ‚úÖ **Yes, but you need 2 things**:

1. Taint the node (`NoSchedule`)
2. **Remove all other scheduling options** (e.g., taint **all other nodes** too, or use **affinity** to force placement)

---

### üõ†Ô∏è Pro Tip: Force Pod onto Tainted Node

To **guarantee** your Pod runs on the tainted node:

```yaml
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values: [k3s-node1]   # your tainted node
  tolerations:
  - key: "color"
    operator: "Equal"
    value: "pink"
    effect: "NoSchedule"

```

---

### ‚û°Ô∏è Summary

‚úÖ **Taints** = node-level **repellents**

‚úÖ **Tolerations** = Pod-level **"immunity"** to taints

‚ö†Ô∏è Tolerant Pods **can still run on untainted nodes**

üîç Use **taints + affinity** to **dedicate nodes**

üõ°Ô∏è k3s master nodes are **tainted by default** ‚Üí Pods won‚Äôt run there