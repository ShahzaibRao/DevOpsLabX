# 6.1: Your First Deployment (`deployment-one.yml`)

https://drive.google.com/file/d/1oUzRfeaB33aht95JwtJKTa7A0d7VrXtB/view?usp=sharing

### ğŸ” YAML Breakdown

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nnwebserver
spec:
  selector:
    matchLabels:
      run: nnwebserver          # â† Must match Pod template labels
  replicas: 2                   # â† Run 2 identical Pods
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1             # â† Allow 1 extra Pod during update
      maxUnavailable: 0       # â† Keep all Pods available during update
  template:
    metadata:
      labels:
        run: nnwebserver       # â† Labels for Pods (must match selector!)
    spec:
      containers:
        - name: nnwebserver
          image: lovelearnlinux/webserver:v2
          livenessProbe:       # â† Health check
            exec:
              command: ["cat", "/var/www/html/index.html"]
            initialDelaySeconds: 10
            periodSeconds: 20
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "200m"
              memory: "256Mi"
          ports:
            - containerPort: 80

```

---

### ğŸ“Œ Why Use a Deployment (vs Bare Pod)?

| Feature | Bare Pod | Deployment |
| --- | --- | --- |
| **Replication** | âŒ 1 Pod only | âœ… `replicas: N` |
| **Self-healing** | âŒ Dies forever | âœ… Restarts failed Pods |
| **Rolling Updates** | âŒ Manual | âœ… Zero-downtime updates |
| **Rollback** | âŒ Impossible | âœ… `kubectl rollout undo` |
| **Scaling** | âŒ Manual | âœ… `kubectl scale` |

> âœ… Deployments manage ReplicaSets, which manage Pods â†’ 3-layer controller pattern.
> 

---

### ğŸ”‘ Key Concepts Explained

### 1. **`selector.matchLabels`**

- **Must exactly match** `template.metadata.labels`
- **Immutable** after creation â†’ donâ€™t change it!

### 2. **RollingUpdate Strategy**

- **`maxSurge: 1`** â†’ During update, allow **1 extra Pod** (total = 3)
- **`maxUnavailable: 0`** â†’ **Zero downtime** (always keep 2 Pods running)

> ğŸ¯ Result:
> 
> - Update starts â†’ 3 Pods (2 old + 1 new)
> - After new Pod is ready â†’ kill 1 old â†’ 3 Pods (1 old + 2 new)
> - Repeat until all updated

### 3. **Liveness Probe**

- Checks if app is **alive**
- If fails â†’ **restart container**
- Uses `exec` to check if `index.html` exists

> ğŸ’¡ Best Practice:
> 
> 
> Also add **`readinessProbe`** to control traffic routing!
> 

### 4. **Resource Requests/Limits**

- Required for **HPA** (Horizontal Pod Autoscaler)
- Enables **QoS classes** (Burstable in this case)

---

### ğŸ§ª k3s Lab: Deploy, Scale, and Update

### ğŸ”§ Step 1: Deploy the Application

```bash
# Apply Deployment
kubectl apply -f deployment-one.yml

# Check status
kubectl get deployments
kubectl get pods -l run=nnwebserver -o wide

# âœ… Expected: 2 Pods running on your k3s nodes

```

### ğŸ”§ Step 2: Inspect the Controller Hierarchy

```bash
# Deployment â†’ ReplicaSet â†’ Pods
kubectl get rs -l run=nnwebserver
# NAME                      DESIRED   CURRENT   READY
# nnwebserver-<hash>        2         2         2

kubectl get pods --show-labels
# Labels include: run=nnwebserver, pod-template-hash=<hash>

```

> ğŸ” pod-template-hash = auto-added by ReplicaSet to track versions.
> 

### ğŸ”§ Step 3: Scale Manually

```bash
# Scale to 3 replicas
kubectl scale deployment nnwebserver --replicas=3

# Verify
kubectl get pods -l run=nnwebserver

```

> âš ï¸ Donâ€™t scale the ReplicaSet directly!
> 
> 
> Deployment will **override** it to match `spec.replicas`.
> 

### ğŸ”§ Step 4: Trigger a Rolling Update

> ğŸ’¡ Change image from v2 â†’ v1 (or vice versa)
> 

```bash
# Edit YAML: image: lovelearnlinux/webserver:v1
kubectl apply -f deployment-one.yml

# Watch rollout
kubectl rollout status deployment nnwebserver

# Check history
kubectl rollout history deployment nnwebserver

```

> ğŸ” Observe:
> 
> - New ReplicaSet created
> - Old ReplicaSet scaled down gradually
> - **Zero downtime** (thanks to `maxUnavailable: 0`)

### ğŸ”§ Step 5: Clean Up

```bash
kubectl delete deployment nnwebserver

```

---

### ğŸ’¡ Pro Tips for k3s

1. **Always set resource requests** â†’ required for HPA and scheduling.
2. **Use `readinessProbe`** to avoid sending traffic to unready Pods:
    
    ```yaml
    readinessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 5
    
    ```
    
3. **Add `strategy.type: Recreate`** for stateful apps that canâ€™t run multiple versions.

---

### â“ Common Questions

**Q: What if I change the `selector` after creation?**

A: âŒ **Forbidden!** Youâ€™ll get: `"selector does not match template labels"`.

**Q: Why use `run` label instead of `app`?**

A: `run` is fine â€” but **`app.kubernetes.io/name`** is [recommended](https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/) for production.

**Q: What happens if liveness probe fails?**

A: Container is **killed and restarted** (check `kubectl describe pod` for `Liveness probe failed`).

**Q: Can I pause a rollout?**

A: âœ… Yes!

```bash
kubectl rollout pause deployment nnwebserver
# Make changes...
kubectl rollout resume deployment nnwebserver

```

---

### â¡ï¸ Summary

âœ… **Deployments** = **replicated, self-healing, updatable** apps.

âœ… **`selector` must match `template.labels`**.

âœ… **RollingUpdate** with `maxUnavailable: 0` = **zero downtime**.

âœ… **Liveness probes** restart unhealthy containers.

ğŸ” In k3s: Works identically to upstream Kubernetes.