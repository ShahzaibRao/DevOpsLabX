# 6.10: HPA with Scaling Policies (Behavior Control)

https://drive.google.com/file/d/16PZAh-epESLHXVZ7VUU0rywuNOes5S-l/view?usp=sharing

### ğŸ” YAML Breakdown

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
meta
  name: nginx-deployment-hpa
spec:
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 30    # â† Wait 30s before scaling down
      policies:
      - type: Pods
        value: 1                        # â† Remove max 1 Pod every 30s
        periodSeconds: 30
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 10          # â† Scale when CPU > 10% of request

```

> ğŸ”‘ Key Insight:
> 
> 
> The `behavior` section lets you **control the speed and stability** of scaling operations.
> 

---

### ğŸ“Œ Scaling Policies Explained

### 1. **`scaleDown` Configuration**

| Field | Meaning | Your Value | Effect |
| --- | --- | --- | --- |
| **`stabilizationWindowSeconds: 30`** | Wait before scaling down | `30s` | Prevents flapping during brief load drops |
| **`policies[0].type: Pods`** | Scale by absolute Pod count | `Pods` | Remove **1 Pod** per interval |
| **`value: 1`** | Max Pods to remove | `1` | Slow, controlled scale-down |
| **`periodSeconds: 30`** | Interval between scale actions | `30s` | One Pod removed every 30 seconds |

> ğŸ¯ Scale-Down Flow:
> 
> - CPU drops below 10% â†’ HPA waits **30s** (`stabilizationWindow`)
> - Then removes **1 Pod**
> - Waits **30s** â†’ removes another (if still underutilized)
> - Until `minReplicas: 3` is reached

### 2. **Missing `scaleUp`?**

> ğŸ’¡ Your YAML only defines scaleDown â†’ scaleUp uses default behavior:
> 
> - **No stabilization window** (scales immediately)
> - **Unlimited rate** (can jump from 3 â†’ 10 instantly)

> âœ… To control scale-up, add:
> 
> 
> ```yaml
> behavior:
>   scaleUp:
>     stabilizationWindowSeconds: 0
>     policies:
>     - type: Percent
>       value: 100    # â† Double replicas every 60s
>       periodSeconds: 60
>   scaleDown:
>     ...
> 
> ```
> 

---

### ğŸ§ª k3s Lab: Observe Scaling Policies in Action

### ğŸ”§ Step 1: Deploy the Full Stack

```bash
# Save as deployment-hpa-with-policies.yaml
kubectl apply -f deployment-hpa-with-policies.yaml

# Verify
kubectl get hpa nginx-deployment-hpa
kubectl get pods -l app=nginx

```

### ğŸ”§ Step 2: Generate Load (Trigger Scale-Up)

```bash
# Start aggressive load (CPU > 10% of 80m = 8m)
kubectl run loader --image=busybox --restart=Never -it -- sh

# Inside shell:
while true; do wget -q -O- <http://nginx-deployment-svc>; done

```

> ğŸ” Watch scale-up (should be fast â€” no policies defined):
> 
> 
> ```bash
> kubectl get hpa -w
> # Replicas jump from 3 â†’ 10 quickly
> 
> ```
> 

### ğŸ”§ Step 3: Stop Load (Trigger Scale-Down)

```bash
# Delete loader (stop load)
kubectl delete pod loader

# Watch scale-down (should be **slow**):
kubectl get hpa -w
# Replicas decrease: 10 â†’ 9 â†’ 8... one every 30s

```

> â±ï¸ Timing:
> 
> - At `t=0s`: Load stops
> - At `t=30s`: HPA removes 1 Pod
> - At `t=60s`: Removes another
> - ... until back to 3 replicas

### ğŸ”§ Step 4: Clean Up

```bash
kubectl delete -f deployment-hpa-with-policies.yaml

```

---

### ğŸ’¡ Real-World Scaling Policies

| Scenario | `scaleUp` Policy | `scaleDown` Policy |
| --- | --- | --- |
| **Web App (User-facing)** | Fast (`+100% every 30s`) | Slow (`-1 Pod every 5m`) |
| **Batch Processor** | Moderate (`+50% every 60s`) | Fast (`-50% every 60s`) |
| **Critical Service** | Immediate (default) | Very slow (`stabilizationWindow: 300s`) |

> âœ… Your config is ideal for:
> 
> - **Cost-sensitive workloads** (slow scale-down saves money)
> - **Stable user experience** (avoids flapping)

---

### ğŸ†š Default vs Policy-Controlled HPA

| Behavior | Default HPA | Your HPA with Policies |
| --- | --- | --- |
| **Scale-Up** | Instant, unlimited | Instant (you didnâ€™t restrict it) |
| **Scale-Down** | After 5m cooldown, fast | After 30s, **1 Pod every 30s** |
| **Flapping Risk** | Medium | **Low** (controlled scale-down) |

> ğŸ’¡ Why 10% CPU target?
> 
> 
> Very aggressive â€” likely for **demo purposes**.
> 
> âœ… **Production recommendation**: `50-70%`
> 

---

### â“ Common Questions

**Q: What if I define multiple policies in `policies[]`?**

A: HPA uses the **most restrictive** (smallest `value`) during scale-down, **most aggressive** during scale-up.

**Q: Can I use `Percent` and `Pods` together?**

A: âœ… Yes! Example:

```yaml
policies:
- type: Percent
  value: 100
  periodSeconds: 60
- type: Pods
  value: 4
  periodSeconds: 60
# â†’ Whichever allows more Pods to be added

```

**Q: Does `stabilizationWindowSeconds` replace the default 5m?**

A: âœ… Yes! Your `30s` **overrides** the default 300s.

**Q: Whatâ€™s the minimum `periodSeconds`?**

A: **15s** (enforced by Kubernetes).

---

### â¡ï¸ Summary

âœ… **`behavior.scaleDown`** = control **how fast** you scale down

âœ… **`stabilizationWindowSeconds`** = delay before scaling down

âœ… **`policies`** = limit rate (Pods or % per period)

ğŸ” In k3s: Test with load generator + `kubectl get hpa -w`

ğŸ› ï¸ **Use slow scale-down** to avoid flapping and save costs