# 6.5: Pod Anti-Affinity in Deployments (HA Spread)

https://drive.google.com/file/d/1L94T05RDk0_rnE1lBvR1q804MPaiDhBp/view?usp=sharing

### ğŸ”§ Corrected & Modernized YAML

```yaml
apiVersion: apps/v1                  # âœ… Modern API (extensions/v1beta1 is DEPRECATED)
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx                    # âœ… Required in apps/v1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                app: nginx          # â† Avoid nodes running Pods with this label
      containers:                    # âœ… Plural "containers" (was "container")
        - name: nginx
          image: nginx:1.25-alpine   # âœ… Avoid :latest
          ports:
            - containerPort: 80

```

> ğŸ”‘ Key Fixes:
> 
> - **`apiVersion: apps/v1`** â†’ current standard (since Kubernetes 1.9)
> - **Added `selector.matchLabels`** â†’ required in `apps/v1`
> - **`containers` (plural)** â†’ correct field name
> - **Specific image tag** â†’ avoid `:latest`

---

### ğŸ“Œ How Anti-Affinity Works in Deployments

| Field | Purpose |
| --- | --- |
| **`podAntiAffinity`** | "Donâ€™t schedule on nodes that match this rule" |
| **`labelSelector.matchLabels`** | Target **your own Pods** (`app: nginx`) |
| **`topologyKey: kubernetes.io/hostname`** | Scope = **same node** |
| **`requiredDuringScheduling...`** | Hard rule â€” **must** spread Pods |

> ğŸ¯ Your Rule:
> 
> 
> *"Do **NOT** schedule two `nginx` Pods on the same node."*
> 

> ğŸ’¡ Why This Matters:
> 
> - If a node fails, **only 1 replica is lost** (not all 3!)
> - Critical for **high availability** in production

---

### ğŸ§ª k3s Lab: Verify Anti-Affinity Spread

> âœ… Assumption: You have â‰¥3 worker nodes in your k3s cluster.
> 
> 
> (If you have only 2 nodes, reduce `replicas: 2`)
> 

### ğŸ”§ Step 1: Deploy the Fixed YAML

```bash
# Save as deployment-with-anti-affinity-fixed.yaml
kubectl apply -f deployment-with-anti-affinity-fixed.yaml

# Wait for Pods to be ready
kubectl get pods -l app=nginx -o wide

```

### ğŸ”§ Step 2: Verify Pod Distribution

```bash
# Check node distribution
kubectl get pods -l app=nginx -o wide

# âœ… Expected (on 3-node cluster):
# NAME                     READY   NODE
# nginx-7df8b9b5d4-abc12   1/1     k3s-node1
# nginx-7df8b9b5d4-def34   1/1     k3s-node2
# nginx-7df8b9b5d4-ghi56   1/1     k3s-node3

# âŒ If you see 2 Pods on same node â†’ anti-affinity failed!

```

### ğŸ”§ Step 3: Test Failure (Not Enough Nodes)

> ğŸ’¡ If you have only 2 nodes but replicas: 3:
> 

```bash
# One Pod will stay Pending!
kubectl get pods -l app=nginx

# Describe to confirm
kubectl describe pod <pending-pod>
# Events: 0/2 nodes available: 2 node(s) didn't match pod anti-affinity rules.

```

> ğŸ” Solution:
> 
> - Add more nodes, OR
> - Use **`preferredDuringScheduling...`** for soft anti-affinity

### ğŸ”§ Step 4: Clean Up

```bash
kubectl delete deployment nginx

```

---

### ğŸ’¡ Pro Tips for k3s

1. **Always use `required` anti-affinity for critical apps**
2. **For zone-level HA** (if multi-AZ):
    
    ```yaml
    topologyKey: topology.kubernetes.io/zone
    
    ```
    
3. **Combine with node anti-affinity** for multi-layer isolation
4. **Verify with `kubectl get pods -o wide`** after every deploy

---

### ğŸ†š Required vs Preferred Anti-Affinity

| Type | Behavior | Use Case |
| --- | --- | --- |
| **`requiredDuringScheduling...`** | Hard rule â€” **blocks scheduling** if violated | Critical apps (databases, payment) |
| **`preferredDuringScheduling...`** | Soft rule â€” **tries to spread**, but wonâ€™t block | Best-effort HA (logging, monitoring) |

> âœ… Your YAML uses required â†’ correct for production.
> 

---

### â“ Common Questions

**Q: What if I have more replicas than nodes?**

A: âŒ **Pods will stay Pending** (with `required` anti-affinity).

âœ… **Fix**: Use `preferred` or add nodes.

**Q: Does anti-affinity work with DaemonSets?**

A: âŒ **No need!** DaemonSets **already run 1 Pod per node**.

**Q: Can I use anti-affinity across namespaces?**

A: âœ… Yes! Add `namespaces: ["other-ns"]` to `labelSelector`.

**Q: Why `kubernetes.io/hostname` and not node name?**

A: `kubernetes.io/hostname` is a **standard label** automatically applied to all nodes.

---

### â¡ï¸ Summary

âœ… **Pod anti-affinity** = **spread replicas across nodes**

âœ… **`requiredDuringScheduling...`** = hard HA requirement

âœ… **`topologyKey: kubernetes.io/hostname`** = per-node spreading

âš ï¸ **Fix deprecated API** â†’ use `apps/v1`

ğŸ” In k3s: Verify with `kubectl get pods -o wide`