# 6.8: Preparing for Horizontal Pod Autoscaler (HPA)

https://drive.google.com/file/d/1Ky19-cY_NotsOW8j36pcgWtcfPOQA9qm/view?usp=sharing

### ğŸ” YAML Breakdown

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8s-autoscaler
spec:
  selector:
    matchLabels:
      run: k8s-autoscaler
  replicas: 2
  template:
    metadata:
      labels:
        run: k8s-autoscaler
    spec:
      containers:
      - name: k8s-autoscaler
        image: lovelearnlinux/webserver:v1
        ports:
        - containerPort: 80
        resources:
          requests:            # â† REQUIRED for HPA
            cpu: "200m"
            memory: "128Mi"
          limits:
            cpu: "500m"
            memory: "256Mi"
---
apiVersion: v1
kind: Service
metadata:
  name: k8s-autoscaler
  labels:
    run: k8s-autoscaler
spec:
  type: ClusterIP
  ports:
  - port: 80
  selector:
    run: k8s-autoscaler

```

> ğŸ”‘ Key Insight:
> 
> 
> **HPA requires `resources.requests`** â€” without it, **autoscaling wonâ€™t work!**
> 

> ğŸ¯ Why?
> 
> 
> HPA calculates **% utilization** as:
> 
> ```
> (Current CPU Usage) / (CPU Request)
> 
> ```
> 
> If no `request` is set â†’ **denominator = 0** â†’ **undefined behavior**.
> 

---

### ğŸ“Œ How HPA Works (CPU-Based)

| Metric | Formula | Your Values |
| --- | --- | --- |
| **Target CPU Utilization** | e.g., 50% | Configured in HPA |
| **Current CPU Usage** | Measured by Metrics Server | e.g., 150m |
| **CPU Request** | From Pod spec | `200m` |
| **Current Utilization** | `150m / 200m = 75%` | > 50% â†’ **scale up!** |

> ğŸ“ˆ Scaling Logic:
> 
> - If **average CPU > target** â†’ **add replicas**
> - If **average CPU < target** â†’ **remove replicas** (after cooldown)

---

### ğŸ§ª k3s Lab: Deploy + Autoscale

> âœ… Prerequisite: Metrics Server must be installed in your k3s cluster.
> 

### ğŸ”§ Step 0: Verify Metrics Server

```bash
# Install if missing (k3s doesn't include it by default)
kubectl apply -f <https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml>

# Verify
kubectl get pods -n kube-system | grep metrics-server
kubectl top nodes
kubectl top pods  # Should show CPU/memory

```

### ğŸ”§ Step 1: Deploy the Application

```bash
# Apply Deployment + Service
kubectl apply -f deployment-for-autoscaler.yaml

# Verify
kubectl get pods -l run=k8s-autoscaler
kubectl top pods  # Should show CPU usage

```

### ğŸ”§ Step 2: Create HPA (CPU-Based)

> ğŸ’¡ Youâ€™ll need hpa-for-autoscaler-deployment.yaml next, but letâ€™s create a basic HPA now:
> 

```bash
# Create HPA: target 50% CPU, min 2, max 5 replicas
kubectl autoscale deployment k8s-autoscaler \\
  --cpu-percent=50 \\
  --min=2 \\
  --max=5

# Verify HPA
kubectl get hpa
# NAME              REFERENCE                TARGETS   MINPODS   MAXPODS   REPLICAS
# k8s-autoscaler    Deployment/k8s-autoscaler   0%/50%    2         5         2

```

### ğŸ”§ Step 3: Generate CPU Load

> ğŸ’¡ Use a stress Pod to generate traffic to your webserver:
> 

```bash
# Start a load generator (runs forever)
kubectl run stress --image=busybox --restart=Never -it -- sh

# Inside the shell:
while true; do wget -q -O- <http://k8s-autoscaler>; done

```

> ğŸ” Open a new terminal to watch scaling:
> 

```bash
# Watch HPA
kubectl get hpa -w

# Watch Pods
kubectl get pods -l run=k8s-autoscaler -w

```

> âœ… Expected:
> 
> - CPU usage rises â†’ HPA shows `75%/50%`
> - Replicas scale from **2 â†’ 3 â†’ 4...** up to **5**

### ğŸ”§ Step 4: Stop Load & Watch Scale-Down

```bash
# Delete stress Pod (stop load)
kubectl delete pod stress

# Watch HPA scale down (after 5 min cooldown)
kubectl get hpa -w

```

> â³ Note: Scale-down has a 5-minute cooldown by default.
> 

### ğŸ”§ Step 5: Clean Up

```bash
kubectl delete hpa k8s-autoscaler
kubectl delete -f deployment-for-autoscaler.yaml

```

---

### ğŸ’¡ Pro Tips for k3s

1. **Always set `resources.requests`** â†’ HPA wonâ€™t work without it.
2. **Use realistic CPU targets**:
    - `-cpu-percent=50` â†’ good for most apps
    - Too low â†’ over-scaling
    - Too high â†’ under-scaling
3. **Test with real load**: `wget`, `hey`, or `ab` (Apache Bench)
4. **Monitor cooldown periods**: Scale-down is **intentionally slow** to avoid flapping.

---

### ğŸ†š HPA v1 vs v2

| Feature | HPA v1 (`autoscale` command) | HPA v2 (YAML) |
| --- | --- | --- |
| **Metrics** | CPU only | CPU, memory, custom, external |
| **Granularity** | Basic | Fine-tuned policies |
| **Use Case** | Quick testing | Production |

> ğŸ’¡ Your next files (hpa-for-deployment-v2.yaml, deployment-hpa-with-policies) will cover HPA v2.
> 

---

### â“ Common Questions

**Q: What if I donâ€™t set `resources.requests`?**

A: âŒ HPA **wonâ€™t work** â€” youâ€™ll see:

`Warning  FailedGetResourceMetric  ... unable to get metrics for resource cpu`

**Q: Can I autoscale on memory?**

A: âœ… Yes! But only with **HPA v2** (your next topic).

**Q: Does HPA work with NodePort/LoadBalancer?**

A: âœ… Yes! HPA watches **Pod metrics**, not Service type.

**Q: Why does scale-down take so long?**

A: Default **stabilization window** = 5 minutes â†’ prevents flapping.

---

### â¡ï¸ Summary

âœ… **`resources.requests`** = **mandatory for HPA**

âœ… HPA scales based on **% of requested resources**

âœ… **Metrics Server** must be installed

ğŸ” In k3s: Test with `kubectl autoscale` + `wget` load

ğŸ› ï¸ Always set **realistic CPU targets** (50% is a good start)