# 6.9: HPA v1 vs HPA v2 â€” Autoscaling Evolution

https://drive.google.com/file/d/15MkFUZqIGv7FbcPxh02Na9ryhe2dBAcE/view?usp=sharing

### ğŸ” HPA v1: Simple CPU Autoscaling

```yaml
# hpa-for-autoscaler-deployment.yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: k8s-autoscaler
spec:
  minReplicas: 2
  maxReplicas: 10
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: k8s-autoscaler
  targetCPUUtilizationPercentage: 10  # â† Scale when CPU > 10% of request

```

> ğŸ”‘ Key Points:
> 
> - **CPU-only**
> - **`targetCPUUtilizationPercentage`** = % of **requested CPU**
> - **Simple, but limited**

> âš ï¸ Why 10%?
> 
> 
> Very aggressive scaling!
> 
> - If Pod requests `200m` CPU â†’ scales when usage > `20m`
> - Good for **bursty workloads**, but may cause **flapping**

---

### ğŸ” HPA v2: Multi-Metric & Flexible

```yaml
# hpa-for-deployment-v2.yaml
apiVersion: autoscaling/v2beta2  # â† Note: v2beta2 (or v2 in 1.23+)
kind: HorizontalPodAutoscaler
metadata:
  name: my-app
spec:
  minReplicas: 1
  maxReplicas: 5
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 66  # â† Scale when CPU > 66% of request

```

> ğŸ”‘ Key Improvements:
> 
> - **`metrics` array** â†’ supports **CPU, memory, custom, external**
> - **`averageUtilization`** â†’ clearer than v1â€™s percentage
> - **Multiple metrics** â†’ scale on **CPU OR memory OR both**

> ğŸ’¡ HPA v2 Structure:
> 
> 
> ```yaml
> metrics:
> - type: Resource       # CPU/memory
>   resource: ...
> - type: Pods           # Custom per-pod metric
>   pods: ...
> - type: Object         # Metric from another object
>   object: ...
> - type: External       # External metric (e.g., queue depth)
>   external: ...
> 
> ```
> 

---

### ğŸ“Œ Critical Notes for k3s

1. **API Version**:
    - Kubernetes **1.23+** â†’ use `autoscaling/v2` (stable)
    - Older â†’ `autoscaling/v2beta2`
    â†’ **k3s v1.28+ supports `v2`**
2. **Metrics Server**:
    
    Must be installed (as we verified earlier).
    
3. **Resource Requests**:
    
    Still **required** for CPU/memory metrics.
    

---

### ğŸ§ª k3s Lab: Compare HPA v1 vs v2

### ğŸ”§ Step 1: Deploy Target Application

> ğŸ’¡ Use your existing deployment-for-autoscaler.yaml (with run: k8s-autoscaler)
> 

```bash
kubectl apply -f deployment-for-autoscaler.yaml

```

### ğŸ”§ Step 2: Apply HPA v1

```bash
# Save as hpa-v1.yaml
kubectl apply -f hpa-for-autoscaler-deployment.yaml

# Watch HPA
kubectl get hpa -w

```

### ğŸ”§ Step 3: Generate Load (Aggressive Scaling)

```bash
# Start load generator
kubectl run loader --image=busybox --restart=Never -it -- sh

# Inside shell:
while true; do wget -q -O- <http://k8s-autoscaler>; done

```

> ğŸ” Observe:
> 
> - With `targetCPUUtilizationPercentage: 10`, replicas scale **very quickly**
> - May jump from 2 â†’ 10 in seconds!

### ğŸ”§ Step 4: Switch to HPA v2 (More Stable)

```bash
# First, update Deployment name to match HPA v2
kubectl patch deployment k8s-autoscaler -p '{"metadata":{"name":"my-app"}}'

# Apply HPA v2
kubectl apply -f hpa-for-deployment-v2.yaml

# Watch scaling (should be slower, more stable)
kubectl get hpa -w

```

> âœ… Expected:
> 
> - Scales at **66% CPU** â†’ fewer replicas, less flapping
> - More suitable for **production**

### ğŸ”§ Step 5: Test Memory Autoscaling (Bonus)

> ğŸ’¡ Create an HPA that scales on memory:
> 

```yaml
# hpa-memory.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-memory
spec:
  minReplicas: 1
  maxReplicas: 5
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  metrics:
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 70

```

```bash
kubectl apply -f hpa-memory.yaml

```

> ğŸ’¡ Generate memory load:
> 
> 
> ```bash
> kubectl exec <pod> -- python3 -c "a='x'*100000000"  # Allocate 100MB
> 
> ```
> 

### ğŸ”§ Step 6: Clean Up

```bash
kubectl delete hpa --all
kubectl delete deployment --all
kubectl delete service --all

```

---

### ğŸ’¡ Real-World HPA Best Practices

| Setting | Recommendation |
| --- | --- |
| **CPU Target** | 50-70% (not 10%!) |
| **Min Replicas** | â‰¥2 (for HA) |
| **Max Replicas** | Based on node capacity |
| **Cooldown** | Use **scaling policies** (next topic) |
| **Metrics** | Combine CPU + memory for stateful apps |

> âœ… Example: Production HPA v2
> 
> 
> ```yaml
> metrics:
> - type: Resource
>   resource:
>     name: cpu
>     target:
>       type: Utilization
>       averageUtilization: 60
> - type: Resource
>   resource:
>     name: memory
>     target:
>       type: Utilization
>       averageUtilization: 80
> 
> ```
> 

---

### â“ Common Questions

**Q: Whatâ€™s the difference between `v2beta2` and `v2`?**

A: `v2` is **stable** (K8s 1.23+). `v2beta2` is older. **Use `v2` if possible**.

**Q: Can I use both CPU and memory in one HPA?**

A: âœ… Yes! HPA scales based on the **metric that needs the most replicas**.

**Q: Why is my HPA not scaling?**

A: Check:

- `kubectl describe hpa` â†’ look for **events**
- `kubectl top pods` â†’ verify metrics are reported
- Ensure **`resources.requests`** are set

**Q: Does HPA work with RollingUpdates?**

A: âœ… Yes! HPA watches **all Pods** (old + new) during updates.

---

### â¡ï¸ Summary

âœ… **HPA v1** = simple CPU autoscaling (`targetCPUUtilizationPercentage`)

âœ… **HPA v2** = multi-metric, stable API (`metrics` array)

âš ï¸ **Always set `resources.requests`**

ğŸ” In k3s: Test with `wget` load + `kubectl get hpa -w`

ğŸ› ï¸ **Use 50-70% CPU target** for production (not 10%!)