# 7.2: Readiness + Liveness Probes (Traffic Control + Auto-Healing)

https://drive.google.com/file/d/1-3S5P8ol2oSQfaSaHRDfWMwdO58tk5TX/view?usp=sharing

### ğŸ” YAML Breakdown

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: probeone
  labels:
    app: apache
spec:
  containers:
  - name: boxone
    image: lovelearnlinux/webserver:v1
    ports:
    - containerPort: 80
    readinessProbe:
      tcpSocket:
        port: 80            # â† Check if TCP port 80 is open
      initialDelaySeconds: 5
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 80            # â† Same check, different purpose
      initialDelaySeconds: 15
      periodSeconds: 20

```

> ğŸ”‘ Key Insight:
> 
> 
> Both probes check the **same TCP port**, but they serve **completely different purposes**.
> 

---

### ğŸ“Œ Readiness vs Liveness: The Critical Difference

| Probe | Purpose | Effect When Fails | Use Case |
| --- | --- | --- | --- |
| **`readinessProbe`** | "Is the app **ready to serve traffic**?" | **Removes Pod from Service endpoints** (no restart) | Startup delay, maintenance mode |
| **`livenessProbe`** | "Is the app **alive**?" | **Restarts the container** | App deadlock, infinite loop, crash |

> ğŸ¯ Your Configuration:
> 
> - **Readiness**: Starts after **5s**, checks every **10s** â†’ fast traffic control
> - **Liveness**: Starts after **15s**, checks every **20s** â†’ slower auto-healing

> ğŸ’¡ Why tcpSocket?
> 
> - Simpler than `httpGet` for basic connectivity
> - Works for **any TCP service** (not just HTTP)

---

### ğŸ§ª k3s Lab: Observe Both Probes in Action

### ğŸ”§ Step 1: Deploy Pod + Service

> ğŸ’¡ We need a Service to see readiness in action.
> 

```bash
# 1. Save Pod YAML as pod-probes.yaml
kubectl apply -f pod-probes.yaml

# 2. Create Service
kubectl expose pod probeone --port=80 --target-port=80

# 3. Verify
kubectl get pods,svc

```

### ğŸ”§ Step 2: Test Readiness (Simulate Startup Delay)

> ğŸ’¡ Block port 80 temporarily to simulate app not ready:
> 

```bash
# On the Pod, block port 80 (simulate app startup)
kubectl exec probeone -- iptables -A INPUT -p tcp --dport 80 -j DROP

# Watch Endpoints (should become <none>)
kubectl get endpoints probeone -w
# NAME       ENDPOINTS   AGE
# probeone   <none>      5s  â† REMOVED FROM SERVICE!

# Test connectivity (should fail)
kubectl run debug --image=curlimages/curl -it --rm -- sh
# curl <http://probeone>  â† Connection refused

```

### ğŸ”§ Step 3: Restore Readiness

```bash
# Unblock port 80
kubectl exec probeone -- iptables -D INPUT -p tcp --dport 80 -j DROP

# Watch Endpoints return
kubectl get endpoints probeone -w
# NAME       ENDPOINTS         AGE
# probeone   10.42.0.10:80     10s  â† BACK IN SERVICE!

# Test connectivity (should work)
curl <http://probeone>  # âœ… Welcome page

```

### ğŸ”§ Step 4: Test Liveness (Simulate App Crash)

> ğŸ’¡ Kill the web server process (but keep container running):
> 

```bash
# Find and kill Apache/nginx process
kubectl exec probeone -- pkill -f apache

# Watch Pod restarts
kubectl get pods probeone -w
# probeone   1/1     Running   1          2m  â† RESTARTED!

# Check why
kubectl describe pod probeone | grep -A 3 "Liveness"
# Warning  Unhealthy  20s  kubelet  Liveness probe failed: dial tcp 10.42.0.10:80: connect: connection refused

```

### ğŸ”§ Step 5: Clean Up

```bash
kubectl delete pod probeone
kubectl delete svc probeone

```

---

### ğŸ’¡ Real-World Probe Strategies

| Scenario | Readiness Probe | Liveness Probe |
| --- | --- | --- |
| **Web App** | `httpGet /health` (checks DB connection) | `httpGet /` (checks basic response) |
| **Database** | `exec pg_isready` | `tcpSocket` on DB port |
| **Queue Worker** | Check queue depth < threshold | Check process is running |
| **Startup Script** | Wait for init script to finish | Monitor main process |

> âœ… Best Practices:
> 
> - **Readiness**: Faster checks, lower `initialDelaySeconds`
> - **Liveness**: Slower checks, higher `failureThreshold` (avoid unnecessary restarts)
> - **Never use liveness probe as readiness** â†’ causes traffic drops during restarts

---

### ğŸ†š Probe Types Comparison

| Type | Use Case | Example |
| --- | --- | --- |
| **`tcpSocket`** | Any TCP service | `port: 80` |
| **`httpGet`** | HTTP/HTTPS apps | `path: /health, port: 80` |
| **`exec`** | Custom logic | `command: ["/app/health.sh"]` |

> ğŸ’¡ Your choice of tcpSocket is perfect for a simple web server.
> 

---

### â“ Common Questions

**Q: What if readiness probe never passes?**

A: Pod stays in **`Running`** state but **never gets traffic** â†’ check logs for startup errors.

**Q: Can I use different ports for readiness and liveness?**

A: âœ… Yes! Example:

```yaml
readinessProbe:
  httpGet:
    path: /ready
    port: 8080
livenessProbe:
  httpGet:
    path: /live
    port: 8080

```

**Q: Does readiness probe affect Pod status?**

A: âŒ No! Pod stays **`Running`** â€” only **Service endpoints** are affected.

**Q: Why not just use liveness probe?**

A: Without readiness:

- Traffic hits Pod **during startup** â†’ 500 errors
- Traffic hits Pod **during shutdown** â†’ dropped requests

---

### â¡ï¸ Summary

âœ… **Readiness probe** = control **traffic routing** (remove from Service)

âœ… **Liveness probe** = **auto-restart** dead containers

âœ… **`tcpSocket`** = simple, effective for TCP services

ğŸ” In k3s: Test with `iptables` block + `kubectl get endpoints`

ğŸ› ï¸ Always use **both probes** for production apps