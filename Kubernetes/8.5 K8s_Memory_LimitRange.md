# 8.5 : Memory-Only LimitRange

https://drive.google.com/file/d/10viXBvrdtGSne5rKc6jwRsoa30IcNgj5/view?usp=sharing

### ğŸ” YAML Breakdown

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: memory-limit-range
  namespace: learning
spec:
  limits:
  - default:              # â† Memory limit if not specified
      memory: 500Mi       # = 500 Mebibytes
    defaultRequest:       # â† Memory request if not specified
      memory: 250Mi       # = 250 Mebibytes
    type: Container

```

> ğŸ”‘ Key Insight:
> 
> 
> This LimitRange **only manages memory** â€” CPU is **not constrained** (Pods can use any CPU unless restricted elsewhere).
> 

> ğŸ’¡ Why memory-only?
> 
> - **Memory is incompressible**: Exceeding limits â†’ **OOMKilled** (no recovery)
> - **CPU is compressible**: Exceeding limits â†’ **throttled** (keeps running)
> â†’ Memory requires **stricter, more predictable limits**

---

### ğŸ“Œ How Memory Units Work

| Unit | Meaning | Notes |
| --- | --- | --- |
| `250Mi` | 250 **Mebibytes** | = 250 Ã— 1024Â² bytes (binary) |
| `250M` | 250 **Megabytes** | = 250 Ã— 1000Â² bytes (decimal) |
| **Kubernetes uses `Mi`/`Gi`** | âœ… **Always prefer binary units** |  |

> âœ… Best Practice:
> 
> 
> Use **`Mi` (Mebibytes)** and **`Gi` (Gibibytes)** for clarity and consistency.
> 

---

### ğŸ§ª k3s Lab: Test Memory-Only LimitRange

### ğŸ”§ Step 1: Create Namespace & Apply LimitRange

```bash
# Create namespace
kubectl create namespace learning

# Apply LimitRange
kubectl apply -f namespace-memory-limitrange.yml

# Verify
kubectl describe limitrange memory-limit-range -n learning
# Type       Resource  Min  Max  Default Request  Default Limit
# Container  memory    -    -    250Mi             500Mi

```

### ğŸ”§ Step 2: Deploy Pod with NO Memory Resources

```yaml
# pod-no-memory.yaml
apiVersion: v1
kind: Pod
metadata:
  name: no-memory-pod
  namespace: learning
spec:
  containers:
  - name: nginx
    image: nginx

```

```bash
kubectl apply -f pod-no-memory.yaml

# Check applied memory resources
kubectl get pod no-memory-pod -n learning -o jsonpath='{.spec.containers[0].resources}'
# âœ… Output:
# {"limits":{"memory":"500Mi"},"requests":{"memory":"250Mi"}}

```

> ğŸ” Result:
> 
> 
> LimitRange **auto-applied memory defaults** â†’ Pod has **250Mi request, 500Mi limit**
> 

### ğŸ”§ Step 3: Trigger OOMKilled (Exceed Memory Limit)

> ğŸ’¡ Deploy a Pod that tries to use 600Mi ( > 500Mi limit):
> 

```yaml
# pod-oom-test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: oom-test
  namespace: learning
spec:
  containers:
  - name: stress
    image: lovelearnlinux/stress:latest
    resources:
      requests:
        memory: "300Mi"
      limits:
        memory: "500Mi"   # â† Hard ceiling
    command: ["stress"]
    args: ["--vm", "1", "--vm-bytes", "600M", "--vm-hang", "1"]

```

```bash
kubectl apply -f pod-oom-test.yaml

# Watch it get killed
kubectl get pods -n learning -w
# oom-test   0/1   OOMKilled   1     20s

# Verify reason
kubectl describe pod oom-test -n learning | grep -A 2 "Last State"
# Last State: Terminated
#   Reason: OOMKilled
#   Exit Code: 137

```

> ğŸ’¥ Key Insight:
> 
> 
> Even though the Pod **defined its own limits**, the LimitRange **didnâ€™t override them** â€” but the **500Mi limit caused OOMKilled** when exceeded.
> 

### ğŸ”§ Step 4: Add `max` to Prevent Over-Requesting (Recommended)

> âš ï¸ Your current LimitRange has no max â†’ users can request 10Gi and exhaust node memory!
> 

```yaml
# Improved LimitRange (with max)
spec:
  limits:
  - default:
      memory: "500Mi"
    defaultRequest:
      memory: "250Mi"
    max:                # â† ADD THIS!
      memory: "1Gi"     # Max 1 GiB per container
    type: Container

```

### ğŸ”§ Step 5: Clean Up

```bash
kubectl delete pod --all -n learning
kubectl delete limitrange memory-limit-range -n learning
kubectl delete namespace learning

```

---

### ğŸ’¡ Real-World Memory LimitRange Strategies

| Workload Type | `defaultRequest` | `default` | `max` | Why |
| --- | --- | --- | --- | --- |
| **Web Server** | 128Mi | 256Mi | 512Mi | Handles traffic spikes |
| **Database** | 1Gi | 2Gi | 4Gi | Needs consistent memory |
| **Cache (Redis)** | 512Mi | 1Gi | 2Gi | Memory = performance |
| **Sandbox** | 64Mi | 128Mi | 256Mi | Prevents node OOM |

> âœ… Best Practices:
> 
> - **Always set `max`** in production
> - **`requests` â‰ˆ average usage**, **`limits` = `requests` + buffer**
> - **Monitor memory usage** (`kubectl top pods`) to tune defaults
> - **Test OOM scenarios** in staging

---

### ğŸ†š Memory vs CPU LimitRange

| Aspect | **Memory LimitRange** | **CPU LimitRange** |
| --- | --- | --- |
| **Enforcement** | OOMKilled (crash) | Throttling (slowed) |
| **Default Strategy** | `requests` â‰ˆ `limits` (no bursting) | `requests` < `limits` (bursting OK) |
| **Risk of No Max** | **Critical** (node instability) | High (node slowdown) |
| **Units** | `Mi`, `Gi` (binary) | `m`, cores (decimal/milli) |

> ğŸ’¡ Golden Rule for Memory:
> 
> 
> **"Set limits you can afford to lose"** â€” because exceeding them **kills your app**.
> 

---

### â“ Common Questions

**Q: What if I set `defaultRequest` > `default`?**

A: âŒ **Invalid!** Kubernetes rejects with:

`requests.memory cannot be greater than limits.memory`

**Q: Does memory request affect scheduling?**

A: âœ… **Yes!** Scheduler ensures node has **â‰¥250Mi free memory** before placing Pod.

**Q: Can I use `M` instead of `Mi`?**

A: âœ… **Yes**, but **`Mi` is standard** (1 MiB = 1024Â², 1 MB = 1000Â²).

**Q: Whatâ€™s the smallest memory request?**

A: **1Mi** â€” but **64Mi+ recommended** for real apps.

---

### â¡ï¸ Summary

âœ… **Memory-only LimitRange** = control memory defaults separately

âœ… **`defaultRequest`** = guaranteed memory (for scheduling)

âœ… **`default`** = max memory (for OOM protection)

âš ï¸ **Always add `max`** to prevent node instability

ğŸ” In k3s: Test with **stress Pod** â†’ watch **OOMKilled**

â¡ï¸ Next: **Comprehensive min/default/max** (`limit-ranges-default-min-max.yaml`)