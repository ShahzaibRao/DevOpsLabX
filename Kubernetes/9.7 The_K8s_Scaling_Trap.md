# 9.7: Deployment with PVC â€” The Hidden Risk

https://drive.google.com/file/d/1Ev4w574TWSojotAOdazZDyMGkcKmkrPA/view?usp=sharing

### ğŸ” YAML Breakdown

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  labels:
    app: nginx
spec:
  replicas: 1                    # â† Only 1 replica (critical!)
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: lovelearnlinux/webserver:v1
        volumeMounts:
        - mountPath: "/var/www/html"
          name: webroot
      volumes:
        - name: webroot
          persistentVolumeClaim:
            claimName: myclaim    # â† References existing PVC

```

> ğŸ”‘ Key Insight:
> 
> 
> This Deployment **mounts a PVC** (`myclaim`) to persist web content.
> 
> âœ… **Safe only because `replicas: 1`**.
> 

> âš ï¸ Critical Warning:
> 
> 
> **Deployments + PVCs = Dangerous with `replicas > 1`!**
> 
> - All replicas **share the same PVC** â†’ **data corruption**
> - PVCs with `ReadWriteOnce` **canâ€™t be mounted on multiple nodes**

---

### ğŸ“Œ When Is This Safe?

| Scenario | Safe? | Why |
| --- | --- | --- |
| **`replicas: 1` + `RWO` PVC** | âœ… Yes | Only one Pod uses the PVC |
| **`replicas: 1` + `RWX` PVC** | âœ… Yes | NFS supports shared access |
| **`replicas > 1` + `RWO` PVC** | âŒ No | Scheduling fails if Pods land on different nodes |
| **`replicas > 1` + `RWX` PVC** | âš ï¸ **Risky** | All Pods write to **same directory** â†’ data collision |

> ğŸ¯ Your YAML is safe because replicas: 1.
> 

---

### ğŸ§ª k3s Lab: Test Deployment with PVC

### ğŸ”§ Step 1: Ensure PVC Exists

> ğŸ’¡ You should already have myclaim from earlier labs:
> 

```bash
kubectl get pvc myclaim
# STATUS = Bound

```

### ğŸ”§ Step 2: Deploy the Application

```bash
kubectl apply -f deployment-using-pvc.yaml

# Verify
kubectl get pods -l app=nginx

```

### ğŸ”§ Step 3: Write Data to Persistent Storage

```bash
# Create a file
kubectl exec webapp-<pod-hash> -- sh -c "echo '<h1>Persistent via Deployment!</h1>' > /var/www/html/index.html"

# Verify
curl http://<service-ip>
# âœ… "Persistent via Deployment!"

```

### ğŸ”§ Step 4: Simulate Pod Failure (Data Survives)

```bash
# Delete Pod (Deployment recreates it)
kubectl delete pod webapp-<pod-hash>

# Verify new Pod serves same data
curl http://<service-ip>
# âœ… Still "Persistent via Deployment!" â†’ data survived!

```

### ğŸ”§ Step 5: Test the Danger (Scale to 2 Replicas)

```bash
# Scale to 2 replicas
kubectl scale deployment webapp --replicas=2

# Watch what happens
kubectl get pods -w

```

> ğŸ” Expected Behavior:
> 
> - If PVC is **`ReadWriteOnce`**:
>     - Second Pod stays in **`Pending`** state
>     - `kubectl describe pod` shows:`waiting for first consumer to be created` or `node(s) had volume node affinity conflict`
> - If PVC is **`ReadWriteMany` (NFS)**:
>     - Both Pods run â†’ **but overwrite each otherâ€™s data!**

### ğŸ”§ Step 6: Clean Up

```bash
kubectl delete deployment webapp
# Keep PVC if you want to reuse it

```

---

### ğŸ’¡ Best Practices for Stateful Workloads

| Pattern | Use Case | Why |
| --- | --- | --- |
| **Deployment + PVC (`replicas: 1`)** | Single-instance apps (CMS, legacy) | Simple, works |
| **StatefulSet + PVC** | Databases, Kafka, ZooKeeper | Stable network IDs, ordered scaling |
| **Deployment + `RWX` PVC (read-only)** | Shared config/templates | Safe if Pods donâ€™t write |

> âœ… For true stateful apps, always prefer StatefulSet:
> 
> 
> ```yaml
> apiVersion: apps/v1
> kind: StatefulSet
> spec:
>   volumeClaimTemplates:  # â† Each Pod gets its OWN PVC
>   - metadata:
>       name: data
>     spec:
>       accessModes: ["ReadWriteOnce"]
>       resources:
>         requests:
>           storage: 10Gi
> 
> ```
> 

---

### ğŸ†š Deployment vs StatefulSet for Storage

| Feature | **Deployment + PVC** | **StatefulSet** |
| --- | --- | --- |
| **PVC per Pod** | âŒ Shared PVC | âœ… Unique PVC per Pod |
| **Stable identity** | âŒ Random names | âœ… `web-0`, `web-1` |
| **Ordered scaling** | âŒ No | âœ… Yes |
| **Use Case** | Single-instance apps | Distributed stateful apps |

---

### â“ Common Questions

**Q: Can I use this for a database?**

A: âŒ **No!** Use **StatefulSet** for databases (MySQL, PostgreSQL).

**Q: What if I need 2+ replicas with shared storage?**

A: âœ… Use **`ReadWriteMany` PVC** (NFS) + **read-only mounts** or **coordinated writes** (e.g., WordPress with NFS).

**Q: Does k3s support StatefulSets?**

A: âœ… **Yes!** Fully supported.

**Q: How do I convert this to a StatefulSet?**

A: Replace `Deployment` with `StatefulSet` and use `volumeClaimTemplates`.

---

### â¡ï¸ Summary

âœ… **Deployment + PVC is safe ONLY with `replicas: 1`**

âš ï¸ **Never scale to `replicas > 1` with shared PVC** (data corruption risk)

ğŸ” In k3s: Test by **scaling to 2 replicas** â†’ observe failure or data collision

ğŸ› ï¸ **Use StatefulSet for true stateful apps**

â¡ï¸ Next: **Service to expose the app** (`service-node-port.yaml`)